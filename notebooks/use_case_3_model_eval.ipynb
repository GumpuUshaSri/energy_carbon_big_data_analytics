{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4bbaa9-7920-41fd-bcc4-9bfa728795eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# MODEL PERFORMANCE EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc523b29-7aa7-4716-83b3-d3fbba4876be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1a2f47-b264-4c61-a971-44f9d9aeebda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType, DoubleType\n",
    "from pyspark.ml.feature import FeatureHasher, VectorAssembler\n",
    "\n",
    "df = spark.table(\"data.energy_volume.energy_power_data\")\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "      .withColumn(\"energy_kWh\", F.col(\"energy_kWh\").cast(DoubleType()))\n",
    "      .withColumn(\"carbon_kg\", F.col(\"carbon_kg\").cast(DoubleType()))\n",
    "      .withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "      .dropna()\n",
    ")\n",
    "\n",
    "# Hash categorical features (low memory)\n",
    "hasher = FeatureHasher(\n",
    "    inputCols=[\"device_type\", \"location\"],\n",
    "    outputCol=\"hashed_features\",\n",
    "    numFeatures=32\n",
    ")\n",
    "\n",
    "hashed = hasher.transform(df)\n",
    "\n",
    "# Add numeric features (this is the fix)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hashed_features\", \"hour\", \"energy_kWh\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "model_data = assembler.transform(hashed).select(\"features\", \"carbon_kg\")\n",
    "\n",
    "train, test = model_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ae2c35-0006-4510-862f-885054a61754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Create modeling dataset(only numeric features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e1d947-d410-4100-a850-f9d9d765b846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "data = assembler.transform(df).select(\"features\", \"carbon_kg\")\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cddb4d80-adf0-4e08-bc5a-9bba8dff2c3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Generalized linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78a2da74-3a19-40bf-b0d6-f5b99af015b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "\n",
    "glm = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"carbon_kg\")\n",
    "glm_model = glm.fit(train)\n",
    "pred_glm = glm_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f572a6ce-c3fe-4848-b6c5-71d504999254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040d85b5-3478-42e7-91b9-71623bc39c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"carbon_kg\", maxDepth=3)\n",
    "dt_model = dt.fit(train)\n",
    "pred_dt = dt_model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf4cc350-bf6c-470c-8875-3c05cdf55daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Small sample GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61d8d326-5930-4da2-b967-861e9c6d884e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "small_train = train.sample(False, 0.08, seed=42)\n",
    "small_test = test.sample(False, 0.08, seed=42)\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"carbon_kg\", maxIter=5, maxDepth=3)\n",
    "gbt_model = gbt.fit(small_train)\n",
    "pred_gbt = gbt_model.transform(small_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4a635e7-f898-4d48-a2bd-b66ae626756d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Isolation forest to detect anamolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae24529b-109b-49f4-98ca-e5f7463bfd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "train_pd = train.toPandas()\n",
    "test_pd = test.toPandas()\n",
    "\n",
    "# Convert 'features' column from DenseVector to numpy array\n",
    "train_features = np.vstack(train_pd[\"features\"].apply(lambda x: np.array(x)))\n",
    "test_features = np.vstack(test_pd[\"features\"].apply(lambda x: np.array(x)))\n",
    "\n",
    "iso = IsolationForest(\n",
    "    contamination=0.03,\n",
    "    random_state=42\n",
    ")\n",
    "iso.fit(train_features)\n",
    "test_pd[\"anomaly_label\"] = iso.predict(test_features)\n",
    "test_pd[\"anomaly_score\"] = iso.decision_function(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e736464-8436-4ef9-9f4f-2443ac5bb260",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1049d978-e4bc-4510-acbb-9ab428d450e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM → RMSE: 0.3919, R²: -0.0005\nDecision Tree → RMSE: 0.3921, R²: -0.0014\nGBT (sampled) → RMSE: 0.3634, R²: -0.0170\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "eval = RegressionEvaluator(labelCol=\"carbon_kg\", predictionCol=\"prediction\")\n",
    "\n",
    "def score(pred, name):\n",
    "    rmse = eval.evaluate(pred, {eval.metricName: \"rmse\"})\n",
    "    r2 = eval.evaluate(pred, {eval.metricName: \"r2\"})\n",
    "    print(f\"{name} → RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "\n",
    "score(pred_glm, \"GLM\")\n",
    "score(pred_dt, \"Decision Tree\")\n",
    "score(pred_gbt, \"GBT (sampled)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "use_case_3_model_eval",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}